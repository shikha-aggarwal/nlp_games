{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "codenames_0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMZzSpoN2EIGFYPzMd7tXDS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shikha-aggarwal/nlp_games/blob/main/codenames_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaXYrVAfTuR4"
      },
      "source": [
        "#An implementation of the board game Codenames.\n",
        "https://en.wikipedia.org/wiki/Codenames_(board_game)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eralVpZo0c8J"
      },
      "source": [
        "## 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLVTvlottqPP",
        "outputId": "2139e763-da41-46f2-9683-f5003f134bfb"
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('words')\n",
        "nltk.download('reuters')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import words"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbpfRXUct5s6",
        "outputId": "5366d44c-f16e-4b5f-f11a-2c1ba29560d6"
      },
      "source": [
        "# The first time you run this will download a ~862MB file\n",
        "glove = torchtext.vocab.GloVe(name=\"6B\", dim=100)\n",
        "porter_stemmer = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "wordlist = words.words()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:52, 2.09MB/s]                           \n",
            "100%|█████████▉| 398349/400000 [00:19<00:00, 19348.20it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwFcI31x-2nw",
        "outputId": "7fefe37f-a400-4528-b0a3-d63de9cb3c7d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PikS5vJ00jal"
      },
      "source": [
        "## 2. Get a set of game-sy words to construct the game. Using just nouns for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SX_ebDhaLoMl",
        "outputId": "48baabc7-8d6e-480a-af83-c0e1ea9d844e"
      },
      "source": [
        "data_dir = '/content/drive/My Drive/Colab Notebooks/Codewords/data/'\n",
        "glove_vocab_file = data_dir + 'vocab.txt'\n",
        "\n",
        "## Save the vocab file only once\n",
        "# def save_glove_vocab(path):\n",
        "#   with open(path, 'w+') as f:     \n",
        "#     for token, index in glove.stoi.items():\n",
        "#       f.write(f'{token}\\n')\n",
        "# save_glove_vocab(glove_vocab_file)\n",
        "\n",
        "def read_glove_vocab(path):\n",
        "    vocab = []\n",
        "    i = 0\n",
        "    with open(path, 'r') as f:\n",
        "      for line in f:\n",
        "        token = line.strip()\n",
        "        vocab.append(token)\n",
        "    return vocab\n",
        "\n",
        "all_glove_words = read_glove_vocab(glove_vocab_file)\n",
        "\n",
        "## Get common-use English words. \n",
        "## Source: https://github.com/first20hours/google-10000-english\n",
        "english_word_files = ['english_10k_long.txt',\n",
        "                      'english_10k_medium.txt']\n",
        "\n",
        "common_use_words = []\n",
        "\n",
        "for filename in english_word_files:\n",
        "    with open(data_dir + filename, \"r\") as file:\n",
        "        for line in file:\n",
        "            word = line.strip()\n",
        "            if word in all_glove_words:\n",
        "                common_use_words.append(word)\n",
        "\n",
        "# Get nouns from wordnet\n",
        "nouns = {x.name().split('.', 1)[0] for x in wn.all_synsets('n')} \n",
        "\n",
        "common_nouns = [w for w in common_use_words if w in nouns]\n",
        "\n",
        "# shuffle the words\n",
        "random.shuffle(common_nouns)\n",
        "print(\"Number of words in our game set: \", len(common_nouns))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words in our game set:  3714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIXRJPzI1zrv"
      },
      "source": [
        "## 3. Util functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvC4Vr7LBzwp",
        "outputId": "733b0042-682f-4722-ef66-88a2f067b2b4"
      },
      "source": [
        "def get_nearest_glove_words(word_vector, n = 5):\n",
        "  \"\"\"\n",
        "  Returns the nearest words in the Glove vector space.\n",
        "\n",
        "  :param word_vector: Glove word vector of the word of type torch.Tensor\n",
        "  :returns: List of tuples (word, distance) in ascending order of distance\n",
        "  \"\"\"\n",
        "  \n",
        "  distance_to_all = torch.norm(glove.vectors - word_vector, dim=1)\n",
        "  dist_sorted = sorted(enumerate(distance_to_all.numpy()), key=lambda x: x[1])\n",
        "  nearest_word_list = []\n",
        "\n",
        "  for index, distance in dist_sorted:\n",
        "    word = glove.itos[index]\n",
        "    if word not in stop_words and word in wordlist:\n",
        "      nearest_word_list.append((word, distance))\n",
        "    if len(nearest_word_list) == n:\n",
        "      break\n",
        "\n",
        "  return nearest_word_list\n",
        "\n",
        "nearest_word_example = get_nearest_glove_words(glove['doctor'] - glove['man'] + glove['woman'])\n",
        "print(\"nearest words in Glove to 'doctor - man + woman': \\n\", nearest_word_example)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nearest words in Glove to 'doctor - man + woman': \n",
            " [('doctor', 3.3640678), ('nurse', 4.2283154), ('physician', 4.7054324), ('woman', 4.8734255), ('dentist', 4.969891)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdhiKWsLGtTG",
        "outputId": "7bcecb45-4f0d-43f5-a3a3-4daf934d2136"
      },
      "source": [
        "def nearest_valid_suggestion(word_group, nearest_words, distance_threshold):\n",
        "  \"\"\"\n",
        "  Filters out words in nearest_words with same root as any of the words in \n",
        "  word_group or more than distance_threshold away.\n",
        "\n",
        "  :param word_group: List of words - fixed points.\n",
        "  :param nearest_words: List of potential points nearby sorted by distance\n",
        "  :param distance_threshold: max distance possible a group word and nearest_word\n",
        "  :returns: nearest_valid_word, distance\n",
        "  \"\"\"\n",
        "  for word, distance in nearest_words:\n",
        "    invalid = False\n",
        "    for w in word_group:\n",
        "      if word in stop_words:\n",
        "        invalid = True\n",
        "      if porter_stemmer.stem(word) == porter_stemmer.stem(w):\n",
        "        invalid = True\n",
        "      if torch.norm(glove[w] - glove[word]) > distance_threshold:\n",
        "        invalid = True\n",
        "\n",
        "    if not invalid:\n",
        "      return word, distance\n",
        "\n",
        "  return None, None\n",
        "\n",
        "\n",
        "def get_clue_word_from_mean(word_list, words_covered = 1, distance_threshold = 4):\n",
        "  \"\"\"\n",
        "  Gets the possible clue words for groups of words_covered number of words\n",
        "  by finding the closest word to the mean of word vectors.\n",
        "\n",
        "  :param word_list: list of words that we need to find clue for.\n",
        "  :param words_covered: number of words we want to try finding the clue for.\n",
        "  :param distance_threshold: max distance possible between clue word and word\n",
        "  :returns: List of tuples (word_group, (word, dist)) in ascending order of dist\n",
        "  \"\"\"\n",
        "  num_words = len(word_list)\n",
        "  distances = {}\n",
        "\n",
        "  nearest_word_list = []\n",
        "\n",
        "  for combination in combinations(word_list, words_covered):\n",
        "    sum_tensor = torch.zeros(glove[word_list[0]].shape)\n",
        "    for w in combination:\n",
        "      sum_tensor += glove[w]\n",
        "    mean_word_vec = sum_tensor/(words_covered * 1.0)\n",
        "    nearest_words = get_nearest_glove_words(mean_word_vec, n = 20)\n",
        "    nearest_valid_word, dist = nearest_valid_suggestion(combination, nearest_words, \n",
        "                                                  distance_threshold)\n",
        "    if nearest_valid_word is not None:\n",
        "      nearest_word_list.append((combination, (nearest_valid_word, dist)))\n",
        "\n",
        "  ## sort according to distance\n",
        "  nearest_word_list = sorted(nearest_word_list, key=lambda x: x[1][1])\n",
        "\n",
        "  return nearest_word_list\n",
        "\n",
        "\n",
        "clues = get_clue_word_from_mean(['doctor', 'man', 'woman', 'grandmother', 'mother', 'king'])\n",
        "print('Possible clues for single words: \\n')\n",
        "for (word, (clue, distance)) in clues:\n",
        "  print(word[0], ' --> ', clue, distance)\n",
        "\n",
        "\n",
        "def get_nearest_word_from_list(word, word_list, n = 5, distance_threshold = 6):\n",
        "  \"\"\"\n",
        "  Returns the nearest word in the word_list.\n",
        "\n",
        "  :param word: string\n",
        "  :param word_list: List of reference words\n",
        "  :returns: nearest word from the list\n",
        "  \"\"\"\n",
        "  zero_tensor = torch.zeros(glove['cat'].shape)\n",
        "\n",
        "  valid_word_list = []\n",
        "  word_list_vectors = []\n",
        "  for w in word_list:\n",
        "    if not torch.all(torch.eq(zero_tensor, glove[w])):\n",
        "      word_list_vectors.append(glove[w])\n",
        "      valid_word_list.append(w)\n",
        "\n",
        "  if len(word_list_vectors) < 1:\n",
        "    return []\n",
        "    \n",
        "  word_list_tensor = torch.stack(word_list_vectors)\n",
        "  \n",
        "  word_vector = glove[word]\n",
        "  distance_to_all = torch.norm(word_list_tensor - word_vector, dim=1)\n",
        "  dist_sorted = sorted(enumerate(distance_to_all.numpy()), key=lambda x: x[1])\n",
        "  nearest_word_list = []\n",
        "\n",
        "  for index, distance in dist_sorted:\n",
        "    invalid = False\n",
        "    cmp_word = valid_word_list[index]\n",
        "    # print(cmp_word, distance)\n",
        "    if cmp_word in stop_words:\n",
        "        invalid = True\n",
        "    if porter_stemmer.stem(word) == porter_stemmer.stem(cmp_word):\n",
        "        invalid = True\n",
        "    if torch.norm(glove[cmp_word] - glove[word]) > distance_threshold:\n",
        "        invalid = True\n",
        "    \n",
        "    if not invalid:\n",
        "      nearest_word_list.append(cmp_word)\n",
        "\n",
        "    if len(nearest_word_list) == n:\n",
        "      break\n",
        "\n",
        "  return nearest_word_list\n",
        "\n",
        "print(\"--------\")\n",
        "print(get_nearest_word_from_list('roti', ['food', 'bread', 'scooter'], n = 5))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Possible clues for single words: \n",
            "\n",
            "grandmother  -->  aunt 2.2975805\n",
            "mother  -->  daughter 2.6008523\n",
            "woman  -->  girl 3.2580621\n",
            "man  -->  woman 3.3640678\n",
            "doctor  -->  physician 3.6094282\n",
            "--------\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylzFIrRIAElP",
        "outputId": "d329c266-fe2e-4f72-b549-8c8d7567e8b4"
      },
      "source": [
        "print(get_nearest_word_from_list('biscuit', ['food', 'bread', 'scooter'], n = 5))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['bread']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdqcGE-qdlfo"
      },
      "source": [
        "from nltk.corpus import reuters\n",
        "reuters_words = reuters.words()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2Zmx-VrrQwX",
        "outputId": "898966cb-cf36-4230-a24d-85315bca13df"
      },
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "def get_lemmas(word):\n",
        "  list_ret = []\n",
        "  for synset in wn.synsets(word):\n",
        "    list_ret.extend(synset.lemma_names())\n",
        "  print(list_ret)\n",
        "  return list_ret\n",
        "\n",
        "def get_clue_word_from_intersection(word_list, words_covered = 1, distance_threshold = 4):\n",
        "  \"\"\"\n",
        "  Gets the possible clue words for groups of words_covered number of words\n",
        "  by finding words in intersection of the nearest words.\n",
        "\n",
        "  :param word_list: list of words that we need to find clue for.\n",
        "  :param words_covered: number of words we want to try finding the clue for.\n",
        "  :param distance_threshold: max distance possible between clue word and word\n",
        "  :returns: List of tuples (word_group, (word, dist)) in ascending order of dist\n",
        "  \"\"\"\n",
        "  combination_to_clue_map = {}\n",
        "\n",
        "  for combination in combinations(word_list, words_covered):\n",
        "    intersection_set = set(get_lemmas(combination[0]))\n",
        "    for w in combination[1:]:\n",
        "      intersection_set = intersection_set.intersection(get_lemmas(w))\n",
        "    combination_to_clue_map[combination] = list(intersection_set)\n",
        "\n",
        "  return combination_to_clue_map\n",
        "\n",
        "print(get_clue_word_from_intersection(['food', 'biscuit', 'bread', 'scooter'], words_covered = 2))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['food', 'nutrient', 'food', 'solid_food', 'food', 'food_for_thought', 'intellectual_nourishment']\n",
            "['biscuit', 'cookie', 'cooky', 'biscuit']\n",
            "['food', 'nutrient', 'food', 'solid_food', 'food', 'food_for_thought', 'intellectual_nourishment']\n",
            "['bread', 'breadstuff', 'staff_of_life', 'boodle', 'bread', 'cabbage', 'clams', 'dinero', 'dough', 'gelt', 'kale', 'lettuce', 'lolly', 'lucre', 'loot', 'moolah', 'pelf', 'scratch', 'shekels', 'simoleons', 'sugar', 'wampum', 'bread']\n",
            "['food', 'nutrient', 'food', 'solid_food', 'food', 'food_for_thought', 'intellectual_nourishment']\n",
            "['water_scooter', 'sea_scooter', 'scooter', 'scooter', 'motor_scooter', 'scooter', 'iceboat', 'ice_yacht', 'scooter', 'scoter', 'scooter']\n",
            "['biscuit', 'cookie', 'cooky', 'biscuit']\n",
            "['bread', 'breadstuff', 'staff_of_life', 'boodle', 'bread', 'cabbage', 'clams', 'dinero', 'dough', 'gelt', 'kale', 'lettuce', 'lolly', 'lucre', 'loot', 'moolah', 'pelf', 'scratch', 'shekels', 'simoleons', 'sugar', 'wampum', 'bread']\n",
            "['biscuit', 'cookie', 'cooky', 'biscuit']\n",
            "['water_scooter', 'sea_scooter', 'scooter', 'scooter', 'motor_scooter', 'scooter', 'iceboat', 'ice_yacht', 'scooter', 'scoter', 'scooter']\n",
            "['bread', 'breadstuff', 'staff_of_life', 'boodle', 'bread', 'cabbage', 'clams', 'dinero', 'dough', 'gelt', 'kale', 'lettuce', 'lolly', 'lucre', 'loot', 'moolah', 'pelf', 'scratch', 'shekels', 'simoleons', 'sugar', 'wampum', 'bread']\n",
            "['water_scooter', 'sea_scooter', 'scooter', 'scooter', 'motor_scooter', 'scooter', 'iceboat', 'ice_yacht', 'scooter', 'scoter', 'scooter']\n",
            "{('food', 'biscuit'): [], ('food', 'bread'): [], ('food', 'scooter'): [], ('biscuit', 'bread'): [], ('biscuit', 'scooter'): [], ('bread', 'scooter'): []}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyCgaRJd2IAy",
        "outputId": "d4cb7f57-cc51-4782-af58-66088a1b7f8b"
      },
      "source": [
        "# lowest_common_hypernyms\n",
        "\n",
        "def get_clue_from_hypernyms_2_words(word1, word2, distance_threshold = 4):\n",
        "    synsets_1 = wn.synsets(word1)\n",
        "    synsets_2 = wn.synsets(word2)\n",
        "\n",
        "    clue_frequency_map = {}\n",
        "\n",
        "    # print(\"length\", len(synsets_1), len(synsets_2))\n",
        "    for synset_1 in synsets_1:\n",
        "        for synset_2 in synsets_2:\n",
        "            paths = synset_1._shortest_hypernym_paths(synset_2)\n",
        "            paths = {k:v for k,v in paths.items() if v > 0}\n",
        "            common_hypernym = min(paths, key=paths.get)\n",
        "            synset_closure = list(min(paths, key=paths.get).closure(lambda s:s.hyponyms()))\n",
        "\n",
        "            # print(\"synsets: \", synset_1, synset_2)\n",
        "            for synset in synset_closure:\n",
        "                for lemma in synset.lemmas():\n",
        "                    # if lemma.name() in all_glove_words:\n",
        "                    clue_frequency_map[lemma.name()] = lemma.count()\n",
        "\n",
        "## refer Glove embeddings to find closest clues\n",
        "# set_1 = set(get_nearest_word_from_list(word_list[0], list(return_dict.keys()), n = 30))\n",
        "# set_2 = set(get_nearest_word_from_list(word_list[1], list(return_dict.keys()), n = 30))      \n",
        "# closest_word_list = list(set_1.intersection(set_2))\n",
        "\n",
        "    return clue_frequency_map\n",
        "\n",
        "print(\"testing function get_clue_from_hypernyms_2_words with words: ['motorcar', 'bike']\")\n",
        "print(get_clue_from_hypernyms_2_words('motorcar', 'bike', 2))\n",
        "print(\"\\n=============\\n\")\n",
        "\n",
        "def get_clue_from_hypernyms(word_list, words_covered = 2, distance_threshold = 4):\n",
        "\n",
        "    combination_to_clue_map = {}\n",
        "    for combination in combinations(word_list, words_covered):\n",
        "        print(\"Evaluating combination: \", combination)\n",
        "        intersection_dict = {}\n",
        "        first_pair = True\n",
        "        clue_frequency_map = {}\n",
        "        for [word1, word2] in combinations(combination, 2):\n",
        "            # print(\"words: \", word1, word2)\n",
        "            clue_frequency_map = get_clue_from_hypernyms_2_words(word1, word2, distance_threshold)\n",
        "            # print(\"clue_frequency_map: \", clue_frequency_map)\n",
        "            if first_pair:\n",
        "                intersection_dict = clue_frequency_map\n",
        "                first_pair = False\n",
        "            else:\n",
        "                intersection_dict = dict(intersection_dict.items() & clue_frequency_map.items())\n",
        "\n",
        "        clue_list_freq_sorted = sorted(list(intersection_dict.keys()), \n",
        "                                       key=lambda x: intersection_dict[x], reverse=True)\n",
        "        \n",
        "        combination_to_clue_map[combination] = clue_list_freq_sorted\n",
        "\n",
        "    return combination_to_clue_map\n",
        "\n",
        "print(get_clue_from_hypernyms(['food', 'biscuit', 'bread', 'scooter'], words_covered = 3))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testing function get_clue_from_hypernyms_2_words with words: ['motorcar', 'bike']\n",
            "{'amphibian': 0, 'amphibious_vehicle': 0, 'bloodmobile': 0, 'car': 71, 'auto': 2, 'automobile': 15, 'machine': 0, 'motorcar': 1, 'doodlebug': 0, 'four-wheel_drive': 0, '4WD': 0, 'go-kart': 0, 'golfcart': 0, 'golf_cart': 0, 'hearse': 0, 'motorcycle': 0, 'bike': 0, 'snowplow': 0, 'snowplough': 0, 'truck': 20, 'motortruck': 0, 'swamp_buggy': 0, 'marsh_buggy': 0, 'ambulance': 3, 'beach_wagon': 0, 'station_wagon': 3, 'wagon': 1, 'estate_car': 0, 'beach_waggon': 0, 'station_waggon': 0, 'waggon': 0, 'bus': 0, 'jalopy': 0, 'heap': 0, 'cab': 0, 'hack': 0, 'taxi': 5, 'taxicab': 0, 'compact': 0, 'compact_car': 0, 'convertible': 2, 'coupe': 0, 'cruiser': 0, 'police_cruiser': 0, 'patrol_car': 0, 'police_car': 3, 'prowl_car': 0, 'squad_car': 1, 'electric': 0, 'electric_automobile': 0, 'electric_car': 0, 'gas_guzzler': 0, 'hardtop': 0, 'hatchback': 0, 'horseless_carriage': 0, 'hot_rod': 1, 'hot-rod': 0, 'jeep': 5, 'landrover': 1, 'limousine': 0, 'limo': 0, 'loaner': 0, 'minicar': 0, 'minivan': 0, 'Model_T': 0, 'pace_car': 0, 'racer': 0, 'race_car': 0, 'racing_car': 0, 'roadster': 0, 'runabout': 0, 'two-seater': 0, 'sedan': 2, 'saloon': 0, 'sport_utility': 0, 'sport_utility_vehicle': 0, 'S.U.V.': 0, 'SUV': 0, 'sports_car': 1, 'sport_car': 0, 'Stanley_Steamer': 0, 'stock_car': 0, 'subcompact': 0, 'subcompact_car': 0, 'touring_car': 0, 'phaeton': 0, 'tourer': 0, 'used-car': 0, 'secondhand_car': 0, 'minibike': 0, 'motorbike': 0, 'trail_bike': 0, 'dirt_bike': 0, 'scrambler': 0, 'dump_truck': 1, 'dumper': 0, 'tipper_truck': 0, 'tipper_lorry': 0, 'tip_truck': 0, 'tipper': 0, 'fire_engine': 0, 'fire_truck': 0, 'garbage_truck': 0, 'dustcart': 0, 'lorry': 0, 'camion': 0, 'pickup': 2, 'pickup_truck': 0, 'sound_truck': 0, 'tow_truck': 0, 'tow_car': 0, 'wrecker': 0, 'tractor': 0, 'trailer_truck': 0, 'tractor_trailer': 0, 'trucking_rig': 0, 'rig': 0, 'articulated_lorry': 0, 'semi': 0, 'transporter': 0, 'car_transporter': 0, 'van': 0, 'funny_wagon': 1, 'shooting_brake': 0, 'gypsy_cab': 0, 'minicab': 0, 'panda_car': 0, 'berlin': 0, 'finisher': 0, 'brougham': 0, 'moped': 0, 'ladder_truck': 0, 'aerial_ladder_truck': 0, 'technical': 0, 'tandem_trailer': 0, 'bookmobile': 0, 'delivery_truck': 0, 'delivery_van': 0, 'panel_truck': 0, 'laundry_truck': 1, 'milk_float': 0, 'moving_van': 0, 'passenger_van': 0, 'police_van': 0, 'police_wagon': 0, 'paddy_wagon': 0, 'patrol_wagon': 0, 'black_Maria': 0, 'pantechnicon': 0}\n",
            "\n",
            "=============\n",
            "\n",
            "Evaluating combination:  ('food', 'biscuit', 'bread')\n",
            "Evaluating combination:  ('food', 'biscuit', 'scooter')\n",
            "Evaluating combination:  ('food', 'bread', 'scooter')\n",
            "Evaluating combination:  ('biscuit', 'bread', 'scooter')\n",
            "{('food', 'biscuit', 'bread'): ['hardtack', 'waffle', 'cookie', 'skillet_bread', 'flannel_cake', 'pilot_biscuit', 'corn_dodger', 'ship_biscuit', 'ratafia_biscuit', 'flapcake', 'refrigerator_cookie', 'muffin', 'raisin-nut_cookie', 'cupcake', 'chocolate_chip_cookie', 'cornbread', 'baking-powder_biscuit', 'doughnut', 'coconut_cake', 'savarin', 'crumb_cake', 'petit_four', 'cruller', 'tostada', 'griddlecake', 'gem', 'sea_biscuit', 'date-nut_bread', 'prune_cake', 'Yorkshire_pudding', 'coffee_cake', 'corn_muffin', 'flannel-cake', 'coffeecake', 'snap', 'Scotch_pancake', 'brownie', 'hot_cake', 'babka', 'Christmas_cake', 'angel_food_cake', 'blini', 'macaroon', 'corn_fritter', 'Belgian_waffle', 'pone', 'simnel', \"devil's_food_cake\", 'fruit_bar', 'latke', 'drop_scone', 'Berlin_doughnut', 'pancake', 'white_cake', 'tortilla', 'flapjack', 'wafer', 'French_pancake', 'Swiss_roll', 'date_bar', 'jelly_doughnut', 'nut_bread', 'jumble', 'donut', 'spice_cookie', 'bran_muffin', 'johnnycake', 'oatmeal_cookie', 'granola_bar', 'jumbal', 'shortcake', 'tea_biscuit', 'genoise', 'popover', 'butter_cookie', 'battercake', 'apple_fritter', 'cornpone', 'johnny_cake', 'journey_cake', 'Madeira_cake', 'banana_bread', 'fry_bread', 'Twinkie', 'hush_puppy', 'ratafia', 'coconut_macaroon', 'gingersnap', 'Toll_House_cookie', 'ginger_nut', 'ginger_snap', \"devil's_food\", 'skillet_cake', 'buttermilk_biscuit', 'birthday_cake', 'torte', 'Madeira_sponge', 'bismark', 'anise_cookie', 'corn_tash', 'spice_cake', 'coffee_ring', 'upside-down_cake', 'oreo', 'French_fritter', 'shortbread_cookie', 'date_bread', 'batter_bread', 'fastnacht', 'brandysnap', 'blintze', 'soda_biscuit', 'sugar_cookie', 'raisin_cookie', 'pound_cake', 'honey_cake', 'layer_cake', 'blintz', 'crumpet', 'spoon_bread', 'fruitcake', 'oreo_cookie', 'almond_crescent', 'buckwheat_cake', 'angel_cake', 'Eccles_cake', 'buttermilk_pancake', 'doughboy', 'hushpuppy', 'applesauce_cake', 'ashcake', 'cheesecake', 'kiss', 'twister', 'Victoria_sandwich', 'shortbread', 'ash_cake', 'cooky', 'scone', 'potato_pancake', 'Victoria_sponge', 'Irish_soda_bread', 'pilot_bread', 'bridecake', 'skillet_corn_bread', 'gingerbread_man', 'jellyroll', 'oatcake', 'almond_cookie', 'crepe', 'molasses_cookie', 'chocolate_cake', 'seed_cake', 'apricot_bar', 'dodger', 'teacake', 'Sally_Lunn', 'chiffon_cake', 'Boston_cream_pie', 'friedcake', 'gateau', 'german_pancake', 'fortune_cookie', 'Shawnee_cake', 'rolled_biscuit', 'bliny', 'gingerbread', 'ladyfinger', 'hoecake', 'baba_au_rhum', 'corn_cake', 'drop_biscuit', 'crape', 'seedcake', 'rum_baba', 'corn_dab', 'wedding_cake', 'crepe_Suzette', 'fritter', 'marble_cake', 'pfannkuchen', 'raised_doughnut', 'hotcake', 'rock_cake', 'sponge_cake', 'dog_biscuit', 'baba', 'sinker', 'beignet'], ('food', 'biscuit', 'scooter'): ['hardtack', 'waffle', 'cookie', 'skillet_bread', 'flannel_cake', 'pilot_biscuit', 'corn_dodger', 'ship_biscuit', 'ratafia_biscuit', 'flapcake', 'refrigerator_cookie', 'muffin', 'raisin-nut_cookie', 'cupcake', 'chocolate_chip_cookie', 'cornbread', 'baking-powder_biscuit', 'doughnut', 'coconut_cake', 'savarin', 'crumb_cake', 'petit_four', 'cruller', 'tostada', 'griddlecake', 'gem', 'sea_biscuit', 'date-nut_bread', 'prune_cake', 'Yorkshire_pudding', 'coffee_cake', 'corn_muffin', 'flannel-cake', 'coffeecake', 'snap', 'Scotch_pancake', 'brownie', 'hot_cake', 'babka', 'Christmas_cake', 'angel_food_cake', 'blini', 'macaroon', 'corn_fritter', 'Belgian_waffle', 'pone', 'simnel', \"devil's_food_cake\", 'fruit_bar', 'latke', 'drop_scone', 'Berlin_doughnut', 'pancake', 'white_cake', 'tortilla', 'flapjack', 'wafer', 'French_pancake', 'Swiss_roll', 'date_bar', 'jelly_doughnut', 'nut_bread', 'jumble', 'donut', 'spice_cookie', 'bran_muffin', 'johnnycake', 'oatmeal_cookie', 'granola_bar', 'jumbal', 'shortcake', 'tea_biscuit', 'genoise', 'popover', 'butter_cookie', 'battercake', 'apple_fritter', 'cornpone', 'johnny_cake', 'journey_cake', 'Madeira_cake', 'banana_bread', 'fry_bread', 'Twinkie', 'hush_puppy', 'ratafia', 'coconut_macaroon', 'gingersnap', 'Toll_House_cookie', 'ginger_nut', 'ginger_snap', \"devil's_food\", 'skillet_cake', 'buttermilk_biscuit', 'birthday_cake', 'torte', 'Madeira_sponge', 'bismark', 'anise_cookie', 'corn_tash', 'spice_cake', 'coffee_ring', 'upside-down_cake', 'oreo', 'French_fritter', 'shortbread_cookie', 'date_bread', 'batter_bread', 'fastnacht', 'brandysnap', 'blintze', 'soda_biscuit', 'sugar_cookie', 'raisin_cookie', 'pound_cake', 'honey_cake', 'layer_cake', 'blintz', 'crumpet', 'spoon_bread', 'fruitcake', 'oreo_cookie', 'almond_crescent', 'buckwheat_cake', 'angel_cake', 'Eccles_cake', 'buttermilk_pancake', 'doughboy', 'hushpuppy', 'applesauce_cake', 'ashcake', 'cheesecake', 'kiss', 'twister', 'Victoria_sandwich', 'shortbread', 'ash_cake', 'cooky', 'scone', 'potato_pancake', 'Victoria_sponge', 'Irish_soda_bread', 'pilot_bread', 'bridecake', 'skillet_corn_bread', 'gingerbread_man', 'jellyroll', 'oatcake', 'almond_cookie', 'crepe', 'molasses_cookie', 'chocolate_cake', 'seed_cake', 'apricot_bar', 'dodger', 'teacake', 'Sally_Lunn', 'chiffon_cake', 'Boston_cream_pie', 'friedcake', 'gateau', 'german_pancake', 'fortune_cookie', 'Shawnee_cake', 'rolled_biscuit', 'bliny', 'gingerbread', 'ladyfinger', 'hoecake', 'baba_au_rhum', 'corn_cake', 'drop_biscuit', 'crape', 'seedcake', 'rum_baba', 'corn_dab', 'wedding_cake', 'crepe_Suzette', 'fritter', 'marble_cake', 'pfannkuchen', 'raised_doughnut', 'hotcake', 'rock_cake', 'sponge_cake', 'dog_biscuit', 'baba', 'sinker', 'beignet'], ('food', 'bread', 'scooter'): ['toast', 'biscuit', 'loaf', 'hardtack', 'unleavened_bread', 'pie', 'cookie', 'English_muffin', 'waffle', 'black_bread', 'sweet_roll', 'loaf_of_bread', 'cracker', 'blueberry_pie', 'lemon_meringue_pie', 'dark_bread', 'staff_of_life', 'corn_dodger', 'ratafia_biscuit', 'muffin', 'cupcake', 'salmon_loaf', 'doughnut', 'baking-powder_biscuit', 'soft_roll', 'napoleon', 'petit_four', 'baguette', 'clover-leaf_roll', 'gem', 'zwieback', 'coffeecake', 'meat_loaf', 'lime', \"shepherd's_pie\", 'snap', 'Scotch_pancake', 'garlic_bread', 'danish_pastry', 'lettuce', 'babka', 'baguet', 'lolly', 'ruggelach', 'macaroon', 'corn_fritter', 'rugelach', 'meatloaf', 'toad-in-the-hole', 'bannock', 'French_pancake', 'mince_pie', 'jumble', 'sour_bread', 'Italian_bread', 'tartlet', 'twice-baked_bread', 'pie_shell', 'banana_bread', 'pumpkin_pie', 'ratafia', 'gingersnap', 'Parker_House_roll', 'torte', 'sticky_bun', 'plate', 'grissino', 'fastnacht', 'whole_meal_bread', 'bear_paw', 'chapatti', 'blintz', 'crumpet', 'spoon_bread', 'almond_crescent', 'caramel_bun', 'water_biscuit', 'hushpuppy', 'applesauce_cake', 'kiss', 'ashcake', 'quiche_Lorraine', 'potato_pancake', 'salt-rising_bread', 'gingerbread_man', 'almond_cookie', 'cinnamon_bun', 'matzoh', 'saltine', 'timbale', 'german_pancake', 'bliny', 'fish_loaf', 'quiche', 'drop_biscuit', 'breadstuff', 'crepe_Suzette', 'coffee_roll', 'fritter', 'marble_cake', 'puff', 'nan', 'rock_cake', 'sponge_cake', 'cinnamon_toast', 'hamburger_roll', 'skillet_bread', 'flannel_cake', 'coconut_cake', 'savarin', 'crumb_cake', 'tostada', 'griddlecake', 'Yorkshire_pudding', 'coffee_cake', 'flannel-cake', 'steak_and_kidney_pie', 'Jewish_rye', 'pone', 'drop_scone', 'pancake', 'flapjack', 'Swiss_roll', 'shoofly_pie', 'jelly_doughnut', 'spread', 'graham_bread', 'chapati', 'hard_roll', 'johnnycake', 'granola_bar', 'pretzel', 'tea_biscuit', 'genoise', 'hallah', 'apple_fritter', 'cornpone', 'chou', 'hush_puppy', 'deep-dish_pie', 'strudel', 'naan', 'rhubarb_pie', 'ginger_snap', 'kidney_pie', 'bismark', 'tart', 'coffee_ring', 'spice_cake', 'corn_tash', 'French_fritter', 'shortbread_cookie', 'flake', 'brandysnap', 'sugar_cookie', 'pound_cake', 'bialystoker', 'eclair', 'bun', 'buckwheat_cake', 'angel_cake', 'buttermilk_pancake', 'flatbread', 'cooky', 'chocolate_eclair', 'scone', 'Victoria_sponge', 'anadama_bread', 'pilot_bread', 'bridecake', 'jellyroll', 'skillet_corn_bread', 'oatcake', 'chocolate_cake', 'apricot_bar', 'Sally_Lunn', 'flatbrod', 'Boston_cream_pie', 'Shawnee_cake', 'pinwheel_roll', 'pecan_pie', 'raisin_bread', 'patty_shell', 'wedding_cake', 'squash_pie', 'patty', 'pfannkuchen', 'pastry', 'sinker', 'kaiser_roll', 'bouchee', 'rye_bread', 'pilot_biscuit', 'Swedish_rye_bread', 'ship_biscuit', 'flapcake', 'refrigerator_cookie', 'brown_bread', 'timbale_case', 'raisin-nut_cookie', 'cruller', 'cinnamon_roll', 'brownie', 'hot_cake', 'Christmas_cake', 'angel_food_cake', 'onion_bread', 'pasty', 'foil', 'Belgian_waffle', 'simnel', \"devil's_food_cake\", 'latke', 'sausage_roll', 'Berlin_doughnut', 'Vienna_roll', 'date_bar', 'onion_bagel', 'nut_bread', 'spice_cookie', 'oatmeal_cookie', 'apple_pie', 'jumbal', 'popover', 'johnny_cake', 'Madeira_cake', 'fry_bread', 'baklava', 'coconut_macaroon', 'Toll_House_cookie', \"devil's_food\", 'birthday_cake', 'meat_pie', 'bialy', 'Madeira_sponge', 'Boston_brown_bread', 'anise_cookie', 'Melba_toast', 'upside-down_cake', 'soda_biscuit', 'dowdy', 'roll', 'apple_tart', 'fruitcake', 'crouton', 'white_bread', 'hamburger_bun', 'Eccles_cake', 'doughboy', 'cheesecake', 'twister', 'ash_cake', 'Jewish_rye_bread', 'rugulah', 'vol-au-vent', 'frangipane', 'cracked-wheat_bread', 'crepe', 'rusk', 'croissant', 'graham_cracker', 'chiffon_cake', 'oyster_cracker', 'gateau', 'pita', 'hot_cross_bun', 'Brussels_biscuit', 'pork_pie', 'onion_roll', 'baba_au_rhum', 'corn_cake', 'crape', 'seedcake', 'corn_dab', 'crescent_roll', 'Swedish_rye', 'sourdough_bread', 'grass', 'raised_doughnut', 'cobbler', 'caraway_seed_bread', 'beignet', 'bagel', 'beigel', 'chocolate_chip_cookie', 'cornbread', 'matzo', 'sea_biscuit', 'date-nut_bread', 'prune_cake', 'corn_muffin', 'streusel', 'challah', 'cream', 'schnecken', 'soft_pretzel', 'blini', 'gluten_bread', 'paste', 'fruit_bar', 'soda_cracker', 'white_cake', 'tortilla', 'pie_crust', 'cream_puff', 'lobster_tart', 'bread-stick', 'French_loaf', 'bear_claw', 'wafer', 'donut', 'pandowdy', 'bran_muffin', 'light_bread', 'whole_wheat_bread', 'shortcake', 'profiterole', 'butter_cookie', 'battercake', 'cake', 'journey_cake', 'scratch', 'quick_bread', 'matzah', 'Twinkie', 'danish', 'ginger_nut', 'skillet_cake', 'buttermilk_biscuit', 'cinnamon_bread', 'oreo', 'brioche', 'date_bread', 'batter_bread', 'blintze', 'tea_bread', 'frankfurter_bun', 'raisin_cookie', 'honey_cake', 'layer_cake', 'pumpernickel', 'oreo_cookie', 'French_bread', 'cross_bun', 'Victoria_sandwich', 'shortbread', 'breadstick', 'cinnamon_snail', 'Irish_soda_bread', 'Cornish_pasty', 'molasses_cookie', 'seed_cake', 'dodger', 'teacake', 'hotdog_bun', 'friedcake', 'bap', 'pocket_bread', 'fortune_cookie', 'French_pastry', 'rolled_biscuit', 'barmbrack', 'gingerbread', 'ladyfinger', 'honey_bun', 'Host', 'tourtiere', 'hoecake', 'rum_baba', 'orange_toast', 'limpa', 'hotcake', 'dog_biscuit', 'baba'], ('biscuit', 'bread', 'scooter'): ['hardtack', 'waffle', 'cookie', 'skillet_bread', 'flannel_cake', 'pilot_biscuit', 'corn_dodger', 'ship_biscuit', 'ratafia_biscuit', 'flapcake', 'refrigerator_cookie', 'muffin', 'raisin-nut_cookie', 'cupcake', 'chocolate_chip_cookie', 'cornbread', 'doughnut', 'baking-powder_biscuit', 'coconut_cake', 'savarin', 'crumb_cake', 'petit_four', 'cruller', 'tostada', 'griddlecake', 'gem', 'sea_biscuit', 'date-nut_bread', 'prune_cake', 'Yorkshire_pudding', 'coffee_cake', 'flannel-cake', 'corn_muffin', 'coffeecake', 'snap', 'Scotch_pancake', 'brownie', 'hot_cake', 'babka', 'Christmas_cake', 'angel_food_cake', 'blini', 'macaroon', 'corn_fritter', 'Belgian_waffle', 'simnel', 'pone', \"devil's_food_cake\", 'fruit_bar', 'latke', 'drop_scone', 'Berlin_doughnut', 'pancake', 'white_cake', 'tortilla', 'flapjack', 'wafer', 'French_pancake', 'Swiss_roll', 'date_bar', 'jelly_doughnut', 'nut_bread', 'jumble', 'donut', 'spice_cookie', 'oatmeal_cookie', 'johnnycake', 'bran_muffin', 'granola_bar', 'jumbal', 'tea_biscuit', 'shortcake', 'genoise', 'popover', 'butter_cookie', 'battercake', 'apple_fritter', 'cornpone', 'johnny_cake', 'journey_cake', 'Madeira_cake', 'banana_bread', 'fry_bread', 'Twinkie', 'hush_puppy', 'ratafia', 'coconut_macaroon', 'gingersnap', 'Toll_House_cookie', 'ginger_nut', 'ginger_snap', \"devil's_food\", 'skillet_cake', 'buttermilk_biscuit', 'birthday_cake', 'torte', 'Madeira_sponge', 'bismark', 'anise_cookie', 'coffee_ring', 'spice_cake', 'oreo', 'upside-down_cake', 'corn_tash', 'French_fritter', 'shortbread_cookie', 'date_bread', 'batter_bread', 'fastnacht', 'brandysnap', 'blintze', 'soda_biscuit', 'sugar_cookie', 'raisin_cookie', 'pound_cake', 'honey_cake', 'layer_cake', 'blintz', 'crumpet', 'fruitcake', 'oreo_cookie', 'spoon_bread', 'almond_crescent', 'buckwheat_cake', 'angel_cake', 'Eccles_cake', 'buttermilk_pancake', 'doughboy', 'cheesecake', 'applesauce_cake', 'kiss', 'twister', 'ashcake', 'hushpuppy', 'Victoria_sandwich', 'shortbread', 'ash_cake', 'cooky', 'scone', 'potato_pancake', 'Victoria_sponge', 'Irish_soda_bread', 'pilot_bread', 'bridecake', 'jellyroll', 'gingerbread_man', 'skillet_corn_bread', 'oatcake', 'almond_cookie', 'crepe', 'molasses_cookie', 'chocolate_cake', 'seed_cake', 'apricot_bar', 'dodger', 'teacake', 'Sally_Lunn', 'chiffon_cake', 'Boston_cream_pie', 'gateau', 'friedcake', 'german_pancake', 'fortune_cookie', 'Shawnee_cake', 'rolled_biscuit', 'bliny', 'gingerbread', 'ladyfinger', 'hoecake', 'baba_au_rhum', 'corn_cake', 'drop_biscuit', 'crape', 'seedcake', 'rum_baba', 'wedding_cake', 'corn_dab', 'crepe_Suzette', 'fritter', 'marble_cake', 'pfannkuchen', 'raised_doughnut', 'hotcake', 'rock_cake', 'sponge_cake', 'dog_biscuit', 'baba', 'sinker', 'beignet']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpSxZjzvkgnA"
      },
      "source": [
        " def get_awesome_for_groups(word_list, word_count = 2):\n",
        "  matching_word = {}\n",
        "  for combination in combinations(word_list, word_count):\n",
        "    matching_word[combination] = get_something_awesome_with_hypernym(combination)\n",
        "  return matching_word"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9GzkLh_EHYz"
      },
      "source": [
        "## 4. Start a Codewords game"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQdIvfUlDzti"
      },
      "source": [
        "## Select words for the game\n",
        "\n",
        "grid_len = 5\n",
        "grid_height = 5\n",
        "num_words_in_game = grid_len * grid_height\n",
        "word_set = random.sample(common_nouns, num_words_in_game)\n",
        "\n",
        "## Divide into red, blue, and neutral\n",
        "one_third = int(num_words_in_game / 3)\n",
        "red_words = random.sample(word_set, one_third)\n",
        "remaining_words = [item for item in word_set if item not in red_words]\n",
        "blue_words = random.sample(remaining_words, one_third)\n",
        "neutral_words = [item for item in remaining_words if item not in blue_words]"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3g_WlxUGHUX"
      },
      "source": [
        "def print_clue_words(words, clues):\n",
        "  print('Words: ', words, '\\n')\n",
        "  print('Possible clues:')\n",
        "  for (word, (clue, distance)) in clues:\n",
        "    print(word[0], ' -- ', clue, distance)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-o358YQFUYz",
        "outputId": "66303f9c-d257-4c58-a7c4-39e94d5beea8"
      },
      "source": [
        "clues = get_clue_word_from_mean(red_words)\n",
        "print_clue_words(red_words, clues)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words:  ['television', 'brisbane', 'smell', 'school', 'lodging', 'diary', 'chain', 'karaoke'] \n",
            "\n",
            "Possible clues:\n",
            "school  --  college 3.1588404\n",
            "television  --  broadcast 3.2479038\n",
            "smell  --  odor 3.2587109\n",
            "brisbane  --  wellington 3.737849\n",
            "lodging  --  accommodation 3.8979847\n",
            "chain  --  supermarket 3.9276807\n",
            "diary  --  memoir 3.9922433\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXmkC50nBB28",
        "outputId": "98652ae0-57e2-4fdb-8d3e-fd7c4270b3fe"
      },
      "source": [
        "clues = get_clue_word_from_mean(red_words, words_covered = 2)\n",
        "print_clue_words(red_words, clues)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words:  ['television', 'brisbane', 'smell', 'school', 'lodging', 'diary', 'chain', 'karaoke'] \n",
            "\n",
            "Possible clues:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LyTw-RKFl4i",
        "outputId": "9fbdf9e6-4812-4220-9a76-e993d50eae2f"
      },
      "source": [
        "clues = get_clue_word_from_mean(blue_words)\n",
        "print_clue_words(blue_words, clues)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words:  ['basis', 'calculation', 'remark', 'michigan', 'apartment', 'bristol', 'armor', 'contributor'] \n",
            "\n",
            "Possible clues:\n",
            "calculation  --  estimation 3.2174172\n",
            "remark  --  suggestion 3.4864097\n",
            "apartment  --  bedroom 3.5394855\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhvjEj7ZFx4Q"
      },
      "source": [
        "clue_pairs = get_awesome_for_groups(red_words, word_count = 2)"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvYfJJFClkMn",
        "outputId": "ba7ded69-3f50-4ebf-9552-91037fb6c87a"
      },
      "source": [
        "for k, v in clue_pairs.items():\n",
        "  print(k, \" : \", v)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('television', 'brisbane')  :  []\n",
            "('television', 'smell')  :  []\n",
            "('television', 'school')  :  []\n",
            "('television', 'lodging')  :  []\n",
            "('television', 'diary')  :  []\n",
            "('television', 'chain')  :  cable\n",
            "('television', 'karaoke')  :  []\n",
            "('brisbane', 'smell')  :  []\n",
            "('brisbane', 'school')  :  []\n",
            "('brisbane', 'lodging')  :  []\n",
            "('brisbane', 'diary')  :  []\n",
            "('brisbane', 'chain')  :  []\n",
            "('brisbane', 'karaoke')  :  []\n",
            "('smell', 'school')  :  []\n",
            "('smell', 'lodging')  :  []\n",
            "('smell', 'diary')  :  []\n",
            "('smell', 'chain')  :  []\n",
            "('smell', 'karaoke')  :  []\n",
            "('school', 'lodging')  :  []\n",
            "('school', 'diary')  :  []\n",
            "('school', 'chain')  :  []\n",
            "('school', 'karaoke')  :  []\n",
            "('lodging', 'diary')  :  guestroom\n",
            "('lodging', 'chain')  :  restaurant\n",
            "('lodging', 'karaoke')  :  diner\n",
            "('diary', 'chain')  :  []\n",
            "('diary', 'karaoke')  :  []\n",
            "('chain', 'karaoke')  :  []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhIyJMBMmzIJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}